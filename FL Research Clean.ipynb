{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8442d7",
   "metadata": {
    "id": "4e8442d7"
   },
   "source": [
    "<h1>Convergence</h1>\n",
    "<ul>\n",
    "    <li>The following code is what was run to perform experiments described in the paper. Feel free to ignore the above code if you are only interested in seeing the final product</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4s2VwUdFq4mU",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:40.196741Z",
     "iopub.status.busy": "2023-05-02T17:11:40.195357Z",
     "iopub.status.idle": "2023-05-02T17:11:50.139166Z",
     "shell.execute_reply": "2023-05-02T17:11:50.141102Z"
    },
    "id": "4s2VwUdFq4mU"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D\n",
    "from tensorflow.keras.regularizers import l1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4f477d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:57.163923Z",
     "iopub.status.busy": "2023-05-02T17:11:57.162278Z",
     "iopub.status.idle": "2023-05-02T17:11:57.169618Z",
     "shell.execute_reply": "2023-05-02T17:11:57.170852Z"
    },
    "id": "d4f477d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU')]\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "#Debugging for GPU acceleration\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "try:\n",
    "    with tf.device('/gpu:0'):\n",
    "        print('GPU is available')\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print('GPU is NOT available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd9f5e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:57.226266Z",
     "iopub.status.busy": "2023-05-02T17:11:57.185986Z",
     "iopub.status.idle": "2023-05-02T17:11:57.620757Z",
     "shell.execute_reply": "2023-05-02T17:11:57.619873Z"
    },
    "id": "dd9f5e1b"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import deque\n",
    "import itertools \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "#Setup for CUDA - but code using GPU has been removed.\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "#Class represents Central Node for MNIST and Fashion MNIST data sets\n",
    "class Central:\n",
    "    def __init__(self,bootstrap_n, budget, model, expected_number_of_participants):\n",
    "        \n",
    "        global global_df\n",
    "        if(len(global_df) < bootstrap_n):\n",
    "            print(\"Error occured, not enough data\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        self.epochs = [2,2,2,2]\n",
    "        self.epoch_ptr = 0\n",
    "\n",
    "        self.budget = budget\n",
    "        \n",
    "        bootstrap_data = []\n",
    "        for i in range(0,10):\n",
    "            subset = global_df[global_df[\"Class\"] == i]\n",
    "            random_sample = subset.sample(bootstrap_n//10)\n",
    "            bootstrap_data.append(random_sample)\n",
    "        self.private_data = pd.concat(bootstrap_data)\n",
    "        global_df = global_df.drop(self.private_data.index)\n",
    "        #Split the data into train and test datasets, with an equal amount of both classes\n",
    "        X = self.private_data.drop(\"Class\", axis = 1)\n",
    "        Y = self.private_data[\"Class\"]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.7, stratify = Y)\n",
    "        #scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_train = X_train.reshape(-1,28,28,1)\n",
    "        X_test = X_test.reshape(-1,28,28,1)\n",
    "        Y_train = keras.utils.to_categorical(Y_train, num_classes = 10)\n",
    "        Y_test = keras.utils.to_categorical(Y_test, num_classes = 10)\n",
    "        self.x_test = X_test\n",
    "        self.y_test = Y_test\n",
    "        self.threshold = 0\n",
    "        \n",
    "        self.model = model\n",
    "        with tf.device('/gpu:0'):\n",
    "            self.model.fit(X_train, Y_train, epochs = 3, validation_data = (X_test, Y_test), verbose = 0)\n",
    "\n",
    "        #For clearing GPU memory\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        self.weights = self.model.get_weights()\n",
    "        self.reward_matrix = dict()\n",
    "        \n",
    "        #For now, base reward is set arbitrarily. \n",
    "        self.base_reward = budget/(expected_number_of_participants*100)\n",
    "    \n",
    "    def flatten(self, lst):\n",
    "        flattened = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(self.flatten(item))\n",
    "            elif isinstance(item, np.ndarray):\n",
    "                flattened.extend(self.flatten(item))\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "    \n",
    "    def register(self, agent):\n",
    "        if agent.id not in self.reward_matrix:\n",
    "            self.reward_matrix[agent.id] = 0\n",
    "    \n",
    "    def broadcast(self):\n",
    "        epochs = self.epochs[self.epoch_ptr]\n",
    "        self.epoch_ptr+=1\n",
    "        if(self.epoch_ptr==4):\n",
    "          self.epoch_ptr = 0\n",
    "        return self.model.get_weights(), self.base_reward, self.reward_matrix, epochs\n",
    "    \n",
    "    def naive_aggregate(self, participant_weights):\n",
    "        flattened_pw = []\n",
    "        for each in participant_weights:\n",
    "            flattened_pw.append(self.flatten(each))\n",
    "        \n",
    "        final_weights = np.average([x for x in flattened_pw], axis = 0)\n",
    "        \n",
    "        #Update the weights of the model\n",
    "        layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "        layer_weights = []\n",
    "        head = 0\n",
    "        \n",
    "        for shape in layer_shapes:\n",
    "            size = np.prod(shape)\n",
    "            current_weight = np.array(final_weights[head:head+size])\n",
    "            layer_weights.append(current_weight.reshape(shape))\n",
    "            head += size\n",
    "        self.model.set_weights(layer_weights)\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def update_model(self, final_weights):\n",
    "        layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "        layer_weights = []\n",
    "        head = 0\n",
    "        \n",
    "        for shape in layer_shapes:\n",
    "            size = np.prod(shape)\n",
    "            current_weight = np.array(final_weights[head:head+size])\n",
    "            layer_weights.append(current_weight.reshape(shape))\n",
    "            head += size\n",
    "        self.model.set_weights(layer_weights)\n",
    "        \n",
    "    \n",
    "\n",
    "    def evaluate(self, weights):\n",
    "      eval_model = self.model\n",
    "      final_weights = np.average([x for x in weights], axis = 0)\n",
    "      \n",
    "      #Update the weights of the model\n",
    "      layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "      layer_weights = []\n",
    "      head = 0\n",
    "      for shape in layer_shapes:\n",
    "          size = (np.prod(shape))\n",
    "          current_weight = np.array(final_weights[head:head+size])\n",
    "          layer_weights.append(current_weight.reshape(shape))\n",
    "          head += (size)\n",
    "      eval_model.set_weights(layer_weights)\n",
    "\n",
    "      return eval_model.evaluate(self.x_test, self.y_test, verbose = 0)[1]\n",
    "\n",
    "        \n",
    "    def strong_cluster(self, participant_weights, n_winners, p_ids):\n",
    "      choice = 5\n",
    "      selection_max = len(participant_weights) * 0.5\n",
    "      while(True):\n",
    "        no_doubles = 1\n",
    "        if(len(participant_weights) < choice):\n",
    "            print(\"clustering failed due to not enough participants\")\n",
    "            return\n",
    "        kmeans = KMeans(n_clusters = choice, random_state = 0, n_init = 10)\n",
    "        flattened_pw = []\n",
    "        for each in participant_weights:\n",
    "            flattened_pw.append(self.flatten(each))\n",
    "        # scaler = StandardScaler()\n",
    "        # scaler.fit(flattened_pw)\n",
    "        # scaled_pw = scaler.transform(flattened_pw)\n",
    "        kmeans.fit(flattened_pw)\n",
    "        \n",
    "    \n",
    "        labels = np.unique(kmeans.labels_)\n",
    "\n",
    "        #Testing purposes only! Remove when not testing\n",
    "        \n",
    "        # Shows ID's of agents in each cluster\n",
    "        label_list = [[] for i in range(max(kmeans.labels_) + 1)]\n",
    "        indices = [[] for i in range(len(labels)+1)]\n",
    "        scoring = [[] for i in range(len(labels)+1)]\n",
    "        remaining_count = 0\n",
    "        for i, label in enumerate(kmeans.labels_):\n",
    "          label_list[label].append(i)\n",
    "          indices[label].append(p_ids[i])\n",
    "          scoring[label].append(i)\n",
    "          remaining_count +=1\n",
    "        print(\"clustering distribution\", indices)\n",
    "        \n",
    "        \n",
    "        accs = []\n",
    "        for i in scoring:\n",
    "          if(len(i) == 0):\n",
    "            continue\n",
    "          accs.append(self.evaluate([flattened_pw[x] for x in i]))\n",
    "#         print(\"individual acc:\", accs)\n",
    "        #End of accuracy testing\n",
    "\n",
    "        #Selecting winners\n",
    "        \n",
    "\n",
    "        #Remove worst cluster\n",
    "        bottom_cluster = np.argmin(accs)\n",
    "        removed_cluster = [x for x in label_list[bottom_cluster]]\n",
    "        label_list[bottom_cluster] = []\n",
    "\n",
    "        #Remove from the weightings at the end of the loop\n",
    "        removal_list = [x for x in label_list if len(x)<=0.1*remaining_count]\n",
    "        removal_list.append(removed_cluster)\n",
    "        removed_labels = [i for i, x in enumerate(label_list) if len(x)<=0.1*remaining_count]\n",
    "        \n",
    "        labels = [x for x in labels if x not in removed_labels]\n",
    "#         print(labels)\n",
    "        \n",
    "        removal_list = [x for i in removal_list for x in i]\n",
    "        removal_list = sorted(removal_list, reverse= True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #Removing clusters with very few members - tau = 0.1n\n",
    "        label_list = [x if len(x)>0.1*remaining_count else [] for x in label_list]\n",
    "\n",
    "\n",
    "\n",
    "        #updating remaining count of participants\n",
    "        remaining_count -= len(removal_list)\n",
    "\n",
    "        #Count clusters remaining\n",
    "        cluster_count = len(labels)\n",
    "        #End clustering if no remaining clusters\n",
    "        if(cluster_count == 0):\n",
    "          print(\"Error occured, no remaining clusters\")\n",
    "          return\n",
    "\n",
    "        #Need to recluster if cluster count is less than Z_min (we set Z_min to 3)\n",
    "        if(cluster_count < 3 and remaining_count>5):\n",
    "          print(\"reclustering\")\n",
    "          for i in removal_list:\n",
    "            participant_weights.pop(i)\n",
    "            p_ids.pop(i)\n",
    "          continue\n",
    "\n",
    "        #In case cluster count is now too small\n",
    "        if(cluster_count<n_winners):\n",
    "          n_winners = cluster_count\n",
    "        \n",
    "        # id_clustering = [p_ids[x] for i in labels for x in label_list[i]]\n",
    "        # print(id_clustering)\n",
    "#         print(label_list)\n",
    "        #- -- - - - - - This section implements Shapley Value - - - -- -  -\n",
    "        possible_coalitions = []\n",
    "        for i in range(1, cluster_count+1):\n",
    "            for combination in itertools.combinations(labels, i):\n",
    "                possible_coalitions.append(combination)\n",
    "        shapley = np.zeros(choice)\n",
    "#         print(possible_coalitions)\n",
    "        for i in labels:\n",
    "            for coalition in possible_coalitions:\n",
    "                if i in coalition:\n",
    "                    if(len(coalition)==1):\n",
    "                        shapley[i]+= self.evaluate([flattened_pw[x] for x in label_list[i]])\n",
    "                    else:\n",
    "                        other_clusters = [label for label in coalition if label!=i]\n",
    "                        other_clusters_pw = [flattened_pw[x] for c in other_clusters for x in label_list[c]]\n",
    "                        full_coalition_pw = [flattened_pw[x] for c in coalition for x in label_list[c]]\n",
    "                        marginal = self.evaluate(full_coalition_pw) - self.evaluate(other_clusters_pw)\n",
    "                        shapley[i] += marginal\n",
    "                        \n",
    "        \n",
    "        print(\"shapley: \", shapley)\n",
    "        #------------- End of section that implements Shapley Value ------------\n",
    "        selected_labels = [cluster_label for cluster_label in labels if shapley[cluster_label] > 0]\n",
    "        selected_indices = [x for i in selected_labels for x in label_list[i]]\n",
    "        winner_ids = [p_ids[i] for i in selected_indices]\n",
    "        print(\"These are the winners\", winner_ids)\n",
    "        selected_parameters = [flattened_pw[x] for x in selected_indices]\n",
    "        break\n",
    "        \n",
    "        #------------- Selection scheme that uses Shapley Value ---------------\n",
    "        \n",
    "\n",
    "#         #Scoring each cluster by accuracy performance; remove any outlier weak clusters\n",
    "#         score = []\n",
    "#         for cluster_label in range(0,len(label_list)):\n",
    "#           heapq.heappush(score, (self.evaluate([flattened_pw[x] for x in label_list[cluster_label]]), cluster_label))\n",
    "        \n",
    "                \n",
    "#         #Best performing clusters\n",
    "#         n_top = heapq.nlargest(n_winners, score, key = lambda x: x[0])\n",
    "\n",
    "#         #Removing clusters with outlier performance\n",
    "#         best_acc = 0\n",
    "#         for each in n_top:\n",
    "#           if(each[0]>best_acc):\n",
    "#             m = each[0]\n",
    "#         n_top = [x for x in n_top if x[0]>= best_acc*0.5] \n",
    "\n",
    "\n",
    "#         selected_labels = [x[1] for x in n_top]\n",
    "#         selected_indices = [x for i in selected_labels for x in label_list[i]]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "#         #Removing excess selected participants\n",
    "#         if(len(selected_indices)>selection_max):\n",
    "#           selected_indices = selected_indices[0:math.ceil(selection_max)]\n",
    "\n",
    "#         #If aggregated model is better than previous then accept changes\n",
    "#         if(self.evaluate([flattened_pw[x] for x in selected_indices]) > self.threshold or self.threshold>0.7):\n",
    "#           winner_ids = [p_ids[i] for i in selected_indices]\n",
    "#           selected_parameters = [flattened_pw[x] for x in selected_indices]\n",
    "#           print(\"winners\", winner_ids)\n",
    "#           break\n",
    "            \n",
    "#         #If re-clustering has occurred too much then just accept \n",
    "#         if(choice>=len(participant_weights)/2):\n",
    "#           winner_ids = [p_ids[i] for i in selected_indices]\n",
    "#           selected_parameters = [flattened_pw[x] for x in selected_indices]\n",
    "#           print(\"winners\", winner_ids)\n",
    "#           print(\"didn't reach n_winners\")\n",
    "#           break\n",
    "#         #Re-cluster but increase number of clusters\n",
    "#         else:\n",
    "#           for i in removal_list:\n",
    "#             participant_weights.pop(i)\n",
    "#             p_ids.pop(i)\n",
    "#           choice+=2\n",
    "#           if(choice>=len(participant_weights)):\n",
    "#             choice = len(participant_weights)-1\n",
    "#           print(\"doubling\")\n",
    "\n",
    "      final_weights = np.average([x for x in selected_parameters], axis = 0)\n",
    "      \n",
    "      #Update the weights of the model\n",
    "      layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "      layer_weights = []\n",
    "      head = 0\n",
    "      \n",
    "      for shape in layer_shapes:\n",
    "          size = np.prod(shape)\n",
    "          current_weight = np.array(final_weights[head:head+size])\n",
    "          layer_weights.append(current_weight.reshape(shape))\n",
    "          head += size\n",
    "      self.model.set_weights(layer_weights)\n",
    "      successful_attacks = 0\n",
    "      for j in winner_ids:\n",
    "        if j<0:\n",
    "            successful_attacks+=1\n",
    "      print(successful_attacks)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "class Participant:\n",
    "    def __init__(self, data_size, participation_cost, identity, model):\n",
    "        \n",
    "        global global_df\n",
    "        if(len(global_df) < data_size):\n",
    "            print(\"Error occured, not enough data\")\n",
    "            return\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.participation_cost = participation_cost\n",
    "        self.weights = []\n",
    "        self.id = identity\n",
    "        self.local_model = tf.keras.models.clone_model(model)\n",
    "        self.epochs = 5\n",
    "        \n",
    "        bootstrap_data = []\n",
    "        for i in range(0,10):\n",
    "            subset = global_df[global_df[\"Class\"] == i]\n",
    "            random_sample = subset.sample(data_size//10)\n",
    "            bootstrap_data.append(random_sample)\n",
    "        self.private_data = pd.concat(bootstrap_data)\n",
    "        global_df = global_df.drop(self.private_data.index)\n",
    "        \n",
    "        \n",
    "    def test_data(self):\n",
    "        print(self.data)\n",
    "\n",
    "    #Need to make a function to decide whether participant will bid to take part in particular round\n",
    "    def make_bid(self):\n",
    "        return 0\n",
    "\n",
    "    def train(self, model_weights, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.local_model.set_weights(model_weights)\n",
    "        self.local_model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "               #Split the data into train and test datasets, with an equal amount of both classes\n",
    "        X = self.private_data.drop(\"Class\", axis = 1)\n",
    "        Y = self.private_data[\"Class\"]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y)\n",
    "        #scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_train = X_train.reshape(-1,28,28,1)\n",
    "        X_test = X_test.reshape(-1,28,28,1)\n",
    "        Y_train = keras.utils.to_categorical(Y_train, num_classes = 10)\n",
    "        Y_test = keras.utils.to_categorical(Y_test, num_classes = 10)\n",
    "        self.local_model.fit(X_train, Y_train,batch_size = 8, validation_data = (X_test, Y_test), epochs = self.epochs, verbose = 0)\n",
    "\n",
    "        self.weights = []\n",
    "        for layer in self.local_model.layers:\n",
    "            layer_w = layer.get_weights()\n",
    "            for w in layer_w:\n",
    "                self.weights.append(w)\n",
    "    \n",
    "    def participate(self, round_reward):\n",
    "        #A simple expected value to decide if it is worth participating - can change to more complicated model soon\n",
    "        if(round_reward > self.participation_cost):\n",
    "            return True\n",
    "    \n",
    "    def submit(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getID(self):\n",
    "      return self.id\n",
    "\n",
    "class Free_Rider:\n",
    "    \n",
    "    def __init__(self, strategy, identity, model):\n",
    "        self.id = identity\n",
    "        self.strategy = strategy\n",
    "        self.local_model = tf.keras.models.clone_model(model)\n",
    "        #History used for delta weight attacks (estimate weight updates)\n",
    "        self.history = None\n",
    "    \n",
    "    def participate(self, round_reward):\n",
    "        return True\n",
    "    \n",
    "    def train(self, model_weights, epochs):\n",
    "        self.local_model.set_weights(model_weights)\n",
    "        self.local_model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "        if self.strategy==1:\n",
    "            #Simply return the global model weights - not used in experiments shown in conference paper\n",
    "            self.weights = []\n",
    "            for layer in self.local_model.layers:\n",
    "                layer_w = layer.get_weights()\n",
    "                for w in layer_w:\n",
    "                    self.weights.append(w)\n",
    "                    \n",
    "        #Random arbitrary noise - not used in experiments shown in conference paper\n",
    "        elif self.strategy == 2:\n",
    "            #return the weights with some arbitrary noise added\n",
    "            noise = random.uniform(random.randint(-15,0),random.randint(1,15))\n",
    "            self.weights = []\n",
    "            for layer in self.local_model.layers:\n",
    "                    layer_w = layer.get_weights()\n",
    "                    for w in layer_w:\n",
    "                        self.weights.append(w+noise)\n",
    "                        \n",
    "        #Gaussian noise\n",
    "        elif self.strategy == 3:\n",
    "            #return the weights with some arbitrary Gaussian noise added\n",
    "            self.weights = []\n",
    "            for layer in self.local_model.layers:\n",
    "                layer_w = layer.get_weights()\n",
    "                for w in layer_w:\n",
    "                    noisy_weights = random.normalvariate(w, w*random.uniform(0, 1.5))\n",
    "                    self.weights.append(noisy_weights)\n",
    "        \n",
    "        #Delta weight attacks\n",
    "        elif self.strategy == 4:\n",
    "            if(self.history == None):\n",
    "                self.history = tf.keras.models.clone_model(self.local_model)\n",
    "                #not enough history yet\n",
    "                self.weights = []\n",
    "                for layer in self.local_model.layers:\n",
    "                    layer_w = layer.get_weights()\n",
    "                    for w in layer_w:\n",
    "                        noisy_weights = random.normalvariate(w, w*random.uniform(0, 1.5))\n",
    "                        self.weights.append(noisy_weights)\n",
    "            else:\n",
    "                self.weights = [] \n",
    "                previous_model = self.history\n",
    "                for layer1, layer2 in zip(self.local_model.layers, previous_model.layers):\n",
    "                    layer_w1 = layer1.get_weights()\n",
    "                    layer_w2 = layer2.get_weights()\n",
    "                    for w1, w2 in zip(layer_w1, layer_w2):\n",
    "                        delta_weights = w1 + (w1-w2)\n",
    "                        delta_noise = random.normalvariate(0, abs(w1-w2)/5)\n",
    "                        self.weights.append(delta_weights + delta_noise)\n",
    "            self.history.set_weights(model_weights)\n",
    "                    \n",
    "    def submit(self):\n",
    "        return self.weights\n",
    "      \n",
    "    def getID(self):\n",
    "      return self.id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f87af53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:57.655101Z",
     "iopub.status.busy": "2023-05-02T17:11:57.635263Z",
     "iopub.status.idle": "2023-05-02T17:11:57.658389Z",
     "shell.execute_reply": "2023-05-02T17:11:57.657556Z"
    },
    "id": "5f87af53"
   },
   "outputs": [],
   "source": [
    "class MaliciousAgent:\n",
    "    def __init__(self, data_size, model, strategy, poison_weight = 0.9, poison_ratio = 0.9, poison_label = 1, target_label=9):\n",
    "        \n",
    "        global global_df\n",
    "        if(len(global_df) < 1000):\n",
    "            print(\"Error occured, not enough data\")\n",
    "            return\n",
    "        \n",
    "        if(strategy == 0):\n",
    "            self.id = -41\n",
    "        elif(strategy == 1):\n",
    "            self.id = -51\n",
    "        elif(strategy == 2):\n",
    "            self.id = -42\n",
    "        elif(strategy == 3):\n",
    "            self.id = -52\n",
    "        elif(strategy == 4):\n",
    "          self.id = -99\n",
    "        \n",
    "        self.target_label = target_label\n",
    "        self.poison_ratio = poison_ratio\n",
    "        self.poison_label = poison_label\n",
    "        self.poison_weight = poison_weight\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        #regularization factor for stealth poisoning\n",
    "        self.rho = 0.1\n",
    "        \n",
    "        self.history = None\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.participation_cost = 0\n",
    "        self.weights = []\n",
    "        self.local_model = tf.keras.models.clone_model(model)\n",
    "        \n",
    "        \n",
    "        bootstrap_data = []\n",
    "        for i in range(0,10):\n",
    "            subset = global_df[global_df[\"Class\"] == i]\n",
    "            random_sample = subset.sample(data_size//10)\n",
    "            bootstrap_data.append(random_sample)\n",
    "        self.private_data = pd.concat(bootstrap_data)\n",
    "        global_df = global_df.drop(self.private_data.index)\n",
    "        self.private_data = self.private_data.reset_index(drop = True)\n",
    "        \n",
    "        self.X = self.private_data.drop(\"Class\", axis = 1)\n",
    "        self.Y = self.private_data[\"Class\"]\n",
    "        \n",
    "\n",
    "    def getID(self):\n",
    "        return self.id\n",
    "    \n",
    "    def participate(self, round_reward):\n",
    "        return True\n",
    "        \n",
    "    \n",
    "    def stealth_loss_function(self, y_true, y_pred):\n",
    "        default_cross_entropy = categorical_crossentropy(y_true, y_pred)\n",
    "        delta_prev = self.current_weights - self.history\n",
    "        delta_mal = self.model.get_weights - self.current_weights \n",
    "        stealth_regularization = self.rho * np.linalg.norm(delta_mal - delta_prev)\n",
    "        return default_cross_entropy + stealth_regularization\n",
    "    \n",
    "    def target_poison(self):\n",
    "        Y = self.Y.copy()\n",
    "        X = self.X.copy()\n",
    "        target_idx = Y[Y == self.target_label].index.tolist()\n",
    "        poisoned_idx = random.sample(target_idx, int(self.poison_ratio*len(target_idx)))\n",
    "        weights = np.ones(self.data_size)\n",
    "        \n",
    "        for idx in poisoned_idx:\n",
    "            Y[idx] = self.poison_label\n",
    "            weights[idx] = self.poison_weight\n",
    "        return X, Y, weights\n",
    "    \n",
    "    def random_poison(self):\n",
    "        Y = self.Y.copy()\n",
    "        X = self.X.copy()\n",
    "        n_poisoned = int(self.poison_ratio*1*self.data_size)\n",
    "        poisoned_idx = np.random.choice(Y.index, n_poisoned, replace = False)\n",
    "        weights = np.ones(self.data_size)\n",
    "        \n",
    "        for idx in poisoned_idx:\n",
    "            true_label = Y[idx]\n",
    "            Y[idx] = random.choice([n for n in range(0,10) if n!=true_label])\n",
    "            weights[idx] = self.poison_weight\n",
    "        return X, Y, weights\n",
    "        \n",
    "        \n",
    "\n",
    "#     def explicit_boost(self, X, Y, weights):\n",
    "#         target_idx = Y[Y == self.target_label].index.tolist()\n",
    "#         n_boosted = int(0.2 * len(target_idx))\n",
    "\n",
    "#         boosted_idx = np.random.choice(target_idx, size=n_boosted)\n",
    "#         print(len(boosted_idx))\n",
    "        \n",
    "#         X_boosted = X[boosted_idx]\n",
    "#         Y_boosted = Y[boosted_idx]\n",
    "#         weights_boosted = weights[boosted_idx]\n",
    "#         X = np.concatenate([X, X_boosted])\n",
    "#         Y = np.concatenate([Y, Y_boosted])\n",
    "#         weights = np.concatenate([weights, weights_boosted])\n",
    "\n",
    "#         return X, Y, weights\n",
    "\n",
    "    def train(self, model_weights, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.current_weights = model_weights\n",
    "        self.local_model.set_weights(model_weights)\n",
    "\n",
    "        #Extreme attack\n",
    "        if(self.strategy==4):\n",
    "          self.weights = []\n",
    "          for layer in self.local_model.layers:\n",
    "            layer_w = layer.get_weights()\n",
    "            for w in layer_w:\n",
    "                self.weights.append(w+random.choice([-99999,99999]))\n",
    "          return \n",
    "\n",
    "        #No stealth\n",
    "        if(self.strategy == 0 or self.strategy == 1):\n",
    "            self.local_model.compile(optimizer = Adam(learning_rate=0.001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "        #Stealth\n",
    "        else:\n",
    "            self.local_model.compile(optimizer = Adam(learning_rate=0.001), loss = self.stealth_loss_function, metrics = [\"accuracy\"])\n",
    "        \n",
    "        #Poisoning attack\n",
    "        #Targeted attack\n",
    "        if(self.strategy ==0 or self.strategy == 2):\n",
    "            X_train, Y_train, weights_train = self.target_poison()\n",
    "        #Random attack\n",
    "        else:\n",
    "            X_train, Y_train, weights_train = self.random_poison()\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test, weights_train, weights_test = train_test_split(X_train, Y_train,weights_train, test_size = 0.1)\n",
    "        #scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_train = X_train.reshape(-1,28,28,1)\n",
    "        X_test = X_test.reshape(-1,28,28,1)   \n",
    "            \n",
    "        Y_test = keras.utils.to_categorical(Y_test, num_classes = 10)    \n",
    "        \n",
    "        Y_train_categorical = keras.utils.to_categorical(Y_train, num_classes = 10)\n",
    "        \n",
    "        model.fit(X_train, Y_train_categorical, batch_size=16, epochs=self.epochs*2, validation_data=(X_test, Y_test),\n",
    "                  sample_weight=weights_train, verbose = 0)\n",
    "\n",
    "#         #explicit boosting\n",
    "#         X_train, Y_train, weights_train = self.explicit_boost(X_train, Y_train, weights_train)\n",
    "#         Y_train_categorical = to_categorical(Y_train)\n",
    "        \n",
    "#         model.fit(X_train, Y_train_categorical, batch_size=16, epochs=2, validation_data=(X_test, Y_test),\n",
    "#                   sample_weight=weights_train, verbose = 0)\n",
    "    \n",
    "        self.weights = []\n",
    "        for layer in self.local_model.layers:\n",
    "            layer_w = layer.get_weights()\n",
    "            for w in layer_w:\n",
    "                self.weights.append(w)\n",
    "    \n",
    "    def submit(self):\n",
    "        return self.weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "217c4632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:57.715937Z",
     "iopub.status.busy": "2023-05-02T17:11:57.693812Z",
     "iopub.status.idle": "2023-05-02T17:11:57.720187Z",
     "shell.execute_reply": "2023-05-02T17:11:57.720815Z"
    },
    "id": "217c4632"
   },
   "outputs": [],
   "source": [
    "## import math \n",
    "\n",
    "def generate_participants(n_participants, cost_mean, cost_var, max_n, min_n, model):\n",
    "    \n",
    "    participants = []\n",
    "    for i in range(n_participants):\n",
    "#         cost = random.normalvariate(cost_mean, cost_var)\n",
    "        cost = 0\n",
    "        data_n = random.randint(min_n, max_n)\n",
    "        a = Participant(data_n, cost, i, model)\n",
    "        participants.append(a)\n",
    "    return participants\n",
    "\n",
    "def generate_participants_CIFAR(n_participants, cost_mean, cost_var, max_n, min_n, model):\n",
    "    participants = []\n",
    "    for i in range(n_participants):\n",
    "        cost = 0\n",
    "        data_n = random.randint(min_n, max_n)\n",
    "        a = Participant_CIFAR(data_n, cost, i, model)\n",
    "        participants.append(a)\n",
    "    return participants\n",
    "\n",
    "#this is used to artificially add free riders to test the CLUSTERING procedure\n",
    "def generate_free_riders(n, model):\n",
    "    free_riders = []\n",
    "    for i in range(math.floor(n/2)):\n",
    "      fr = Free_Rider(4, -3, model)\n",
    "      free_riders.append(fr)\n",
    "    for i in range(math.ceil(n/2)):\n",
    "      fr = Free_Rider(3, -2, model)\n",
    "      free_riders.append(fr)\n",
    "    return free_riders\n",
    "\n",
    "def generate_free_riders_CIFAR(n, model):\n",
    "    free_riders = []\n",
    "    for i in range(math.floor(n/2)):\n",
    "      fr = Free_Rider_CIFAR(4, -3, model)\n",
    "      free_riders.append(fr)\n",
    "    for i in range(math.ceil(n/2)):\n",
    "      fr = Free_Rider_CIFAR(3, -2, model)\n",
    "      free_riders.append(fr)\n",
    "    return free_riders\n",
    "\n",
    "\n",
    "def add_fr(distributed_nodes, n, model):\n",
    "    fr = generate_free_riders(n, model)\n",
    "    for each in fr:\n",
    "        distributed_nodes.append(each)\n",
    "\n",
    "def add_fr_CIFAR(distributed_nodes, n, model):\n",
    "    fr = generate_free_riders_CIFAR(n, model)\n",
    "    for each in fr:\n",
    "        distributed_nodes.append(each) \n",
    "\n",
    "        \n",
    "def generate_malicious_attackers(n, model):\n",
    "    malicious_attackers = []\n",
    "    if(n==1):\n",
    "        m = MaliciousAgent(1000, model, 0)\n",
    "        malicious_attackers.append(m)\n",
    "        return malicious_attackers\n",
    "    \n",
    "    for i in range(math.floor(n/2)):\n",
    "      m = MaliciousAgent(500, model, 0)\n",
    "      malicious_attackers.append(m)\n",
    "    for i in range(math.ceil(n/2)):\n",
    "      m = MaliciousAgent(500, model, 1)\n",
    "      malicious_attackers.append(m)\n",
    "    return malicious_attackers\n",
    "\n",
    "def generate_malicious_attackers_CIFAR(n, model):\n",
    "    malicious_attackers = []\n",
    "    if(n==1):\n",
    "        m = MaliciousAgent_CIFAR(1000, model, 0)\n",
    "        malicious_attackers.append(m)\n",
    "        return malicious_attackers\n",
    "    \n",
    "    for i in range(math.floor(n/2)):\n",
    "      m = MaliciousAgent_CIFAR(500, model, 0)\n",
    "      malicious_attackers.append(m)\n",
    "    for i in range(math.ceil(n/2)):\n",
    "      m = MaliciousAgent_CIFAR(500, model, 1)\n",
    "      malicious_attackers.append(m)\n",
    "    return malicious_attackers\n",
    "\n",
    "def add_ma_each(distributed_nodes, model):\n",
    "    m1 = MaliciousAgent(500, model, 0)\n",
    "    m2 = MaliciousAgent(500, model, 1)\n",
    "    m3 = MaliciousAgent(500, model, 2)\n",
    "    m4 = MaliciousAgent(500, model, 3)\n",
    "    distributed_nodes.append(m1)\n",
    "    distributed_nodes.append(m2)\n",
    "    distributed_nodes.append(m3)\n",
    "    distributed_nodes.append(m4)\n",
    "    \n",
    "\n",
    "def add_ma_general_naive(distributed_nodes, model):\n",
    "  ma = MaliciousAgent(500, model, 1)\n",
    "  distributed_nodes.append(ma)\n",
    "def add_ma_target_naive(distributed_nodes, model):\n",
    "  ma = MaliciousAgent(500, model, 0)\n",
    "  distributed_nodes.append(ma)\n",
    "\n",
    "def add_ma_naive(distributed_nodes, model):\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  \n",
    "def add_ma_general_stealth(distributed_nodes, model):\n",
    "  ma = MaliciousAgent(500, model, 3)\n",
    "  distributed_nodes.append(ma)\n",
    "def add_ma_target_stealth(distributed_nodes, model):\n",
    "  ma = MaliciousAgent(500, model, 2)\n",
    "  distributed_nodes.append(ma)  \n",
    "def add_ma_stealth(distributed_nodes, model):\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "\n",
    "def add_ma(distributed_nodes, n, model):\n",
    "    ma = generate_malicious_attackers(n, model)\n",
    "    for each in ma:\n",
    "        distributed_nodes.append(each)\n",
    "        \n",
    "def add_ma_CIFAR(distributed_nodes, n, model):\n",
    "    ma = generate_malicious_attackers_CIFAR(n, model)\n",
    "    for each in ma:\n",
    "        distributed_nodes.append(each)\n",
    "\n",
    "def federated_round(central_node, distributed_nodes, n_winners):\n",
    "    \n",
    "    #Initialise new distributed nodes in central node\n",
    "    for each in distributed_nodes:\n",
    "        central_node.register(each)\n",
    "    \n",
    "    round_model, round_reward, reward_matrix, epochs = central_node.broadcast()\n",
    "    round_participants = []\n",
    "    #Distributed nodes decide if they wish to participate\n",
    "    round_participants = distributed_nodes #for experiment, say they all participate.\n",
    "    participant_weights = []\n",
    "    participant_ids = []\n",
    "    #Broadcast global round model and training for all participants\n",
    "    for node in round_participants:\n",
    "        node.train(round_model, epochs)\n",
    "        participant_weights.append(node.submit())\n",
    "        participant_ids.append(node.getID())\n",
    "#     print(\"clustering\")\n",
    "    #Cluster, select winners, aggregate weights - May need to extend to add payments --> not sure if this is necssary\n",
    "    central_node.strong_cluster(participant_weights, n_winners, participant_ids)\n",
    "    \n",
    "\n",
    "def naive_federated_round(central_node, distributed_nodes, n_winners):\n",
    "    participant_weights = []\n",
    "    participant_ids = []\n",
    "    for each in distributed_nodes:\n",
    "        central_node.register(each)\n",
    "    round_model, round_reward, reward_matrix, epochs = central_node.broadcast()\n",
    "    round_participants = random.sample(distributed_nodes, math.ceil(0.3*len(distributed_nodes)))\n",
    "    for node in round_participants:\n",
    "        node.train(round_model, epochs)\n",
    "        participant_weights.append(node.submit())\n",
    "        participant_ids.append(node.getID())\n",
    "    central_node.naive_aggregate(participant_weights)\n",
    "\n",
    "def n_rounds(n, central_node, distributed_nodes, n_winners):\n",
    "    accuracies = []\n",
    "    for i in range(0,n):\n",
    "        federated_round(central_node, distributed_nodes, n_winners)\n",
    "        acc = test(central_node)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "def n_rounds_CIFAR(n, central_node, distributed_nodes, n_winners):\n",
    "    accuracies = []\n",
    "    for i in range(0,n):\n",
    "        federated_round(central_node, distributed_nodes, n_winners)\n",
    "        acc = test_CIFAR(central_node)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def naive_rounds(n, central_node, distributed_nodes, n_winners):\n",
    "    accuracies = []\n",
    "    for i in range(0,n):\n",
    "        naive_federated_round(central_node, distributed_nodes, n_winners)\n",
    "        acc = test(central_node)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "def naive_rounds_CIFAR(n, central_node, distributed_nodes, n_winners):\n",
    "    accuracies = []\n",
    "    for i in range(0,n):\n",
    "        naive_federated_round(central_node, distributed_nodes, n_winners)\n",
    "        acc = test_CIFAR(central_node)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "def n_rounds_fd(n, central_node, distributed_nodes, n_winners):\n",
    "    accuracies = []\n",
    "    for i in range(0,n):\n",
    "        federated_round(central_node, distributed_nodes, n_winners)\n",
    "        acc = test_fraud_detection(central_node)\n",
    "        if(acc>=90):\n",
    "          print(\"Converged to 90 in \", i)\n",
    "          break\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "#testing purposes only\n",
    "def test(central_node):\n",
    "    test_x = test_df.drop(\"Class\", axis = 1).to_numpy()\n",
    "    test_y = test_df[\"Class\"]\n",
    "    scaler = StandardScaler()\n",
    "    test_x = scaler.fit_transform(test_x)\n",
    "    test_x = test_x.reshape(-1,28,28,1)\n",
    "    test_y = keras.utils.to_categorical(test_y, num_classes = 10)\n",
    "    local_model = central_node.get_model()\n",
    "    loss, accuracy = local_model.evaluate(test_x, test_y)\n",
    "    print(loss, accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def test_CIFAR(central_node):\n",
    "    test_x = test_df.drop(\"Class\", axis = 1).to_numpy()\n",
    "    test_y = test_df[\"Class\"]\n",
    "    scaler = StandardScaler()\n",
    "    test_x = scaler.fit_transform(test_x)\n",
    "    test_x = np.reshape(test_x, (test_x.shape[0], 32, 32, 3))\n",
    "    test_y = keras.utils.to_categorical(test_y, num_classes = 10)\n",
    "    local_model = central_node.get_model()\n",
    "    loss, accuracy = local_model.evaluate(test_x, test_y)\n",
    "    print(loss, accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def test_fraud_detection(central_node):\n",
    "    test_x = test_df.drop(\"Class\", axis = 1).to_numpy()\n",
    "    test_y = test_df[\"Class\"]\n",
    "    scaler = StandardScaler()\n",
    "    test_x = scaler.fit_transform(test_x)\n",
    "    test_x = test_x.reshape(test_x.shape[0], test_x.shape[1], 1)\n",
    "    local_model = central_node.get_model()\n",
    "    loss, accuracy = local_model.evaluate(test_x, test_y)\n",
    "    print(loss, accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97569ca",
   "metadata": {
    "id": "b97569ca"
   },
   "source": [
    "<h2>Below separates data so we can validate the results of the federated learning model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fab3c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:57.727016Z",
     "iopub.status.busy": "2023-05-02T17:11:57.726267Z",
     "iopub.status.idle": "2023-05-02T17:11:57.729251Z",
     "shell.execute_reply": "2023-05-02T17:11:57.728501Z"
    },
    "id": "5fab3c44"
   },
   "outputs": [],
   "source": [
    "# #Credit card data\n",
    "# data = pd.read_csv('creditcard.csv')\n",
    "# X = data.drop(\"Class\", axis = 1)\n",
    "# Y = data[\"Class\"]\n",
    "# #Global X and Y is the 'total dataset', from which samples will be taken to simulate data of participants\n",
    "# #Test X and Y is the test dataset from which we use to test the performance of the model on unseen data.\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# global_df = pd.concat([X_train, y_train], axis=1)\n",
    "# test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_X_train = scaler.fit_transform(X_train)\n",
    "# scaled_X_train = scaled_X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "\n",
    "# reshape_form_train = (X_train.shape[0], X_train.shape[1], 1)\n",
    "# reshape_form_test = (X_test.shape[0], X_test.shape[1], 1)\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(32, 2 ,activation = \"relu\", input_shape = scaled_X_train[0].shape, kernel_regularizer=l1(0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(64, 2 ,activation = \"relu\", kernel_regularizer=l1(0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(64,activation = \"relu\", kernel_regularizer=l1(0.01)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64,activation = \"relu\", kernel_regularizer=l1(0.01)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(1, activation = \"sigmoid\"))\n",
    "# model.summary()\n",
    "# model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd3406e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:57.743345Z",
     "iopub.status.busy": "2023-05-02T17:11:57.738010Z",
     "iopub.status.idle": "2023-05-02T17:11:58.521750Z",
     "shell.execute_reply": "2023-05-02T17:11:58.521051Z"
    },
    "id": "cd3406e3",
    "outputId": "23915aa8-5783-451b-e5b3-cac675809c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 11, 11, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,418\n",
      "Trainable params: 225,226\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#MNIST data\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# X_train = X_train/255.0\n",
    "# X_test = X_test /255.0\n",
    "\n",
    "global_df = pd.DataFrame(X_train)\n",
    "global_df['Class'] = y_train\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['Class'] = y_test\n",
    "\n",
    "reshape_form_train = (28,28)\n",
    "reshape_form_test = (28,28)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_X_train = scaler.fit_transform(X_train)\n",
    "# scaled_X_train = scaled_X_train.reshape(reshape_form_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3) ,activation = \"relu\", input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3) ,activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation = \"relu\", kernel_regularizer=l1(0.2)))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "model.summary()\n",
    "model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "074b6f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:58.533852Z",
     "iopub.status.busy": "2023-05-02T17:11:58.532865Z",
     "iopub.status.idle": "2023-05-02T17:11:58.534842Z",
     "shell.execute_reply": "2023-05-02T17:11:58.535575Z"
    },
    "id": "074b6f5b"
   },
   "outputs": [],
   "source": [
    "def call_mnist():\n",
    "    global global_df\n",
    "    global test_df\n",
    "    global model\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "    # X_train = X_train/255.0\n",
    "    # X_test = X_test /255.0\n",
    "\n",
    "    global_df = pd.DataFrame(X_train)\n",
    "    global_df['Class'] = y_train\n",
    "    test_df = pd.DataFrame(X_test)\n",
    "    test_df['Class'] = y_test\n",
    "\n",
    "    reshape_form_train = (28,28)\n",
    "    reshape_form_test = (28,28)\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # scaled_X_train = scaler.fit_transform(X_train)\n",
    "    # scaled_X_train = scaled_X_train.reshape(reshape_form_train)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3) ,activation = \"relu\", input_shape = (28,28,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(64, (3,3) ,activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128,activation = \"relu\", kernel_regularizer=l1(0.2)))\n",
    "    model.add(Dense(10, activation = \"softmax\"))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb37dc7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:58.547239Z",
     "iopub.status.busy": "2023-05-02T17:11:58.546029Z",
     "iopub.status.idle": "2023-05-02T17:11:59.511352Z",
     "shell.execute_reply": "2023-05-02T17:11:59.511847Z"
    },
    "id": "fb37dc7a",
    "outputId": "7c851a0f-0a7f-4a9f-82d2-9f7e7321c86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 26, 26, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 11, 11, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 3, 3, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 242,442\n",
      "Trainable params: 241,994\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# X_train = X_train/255.0\n",
    "# X_test = X_test /255.0\n",
    "\n",
    "global_df = pd.DataFrame(X_train)\n",
    "global_df['Class'] = y_train\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['Class'] = y_test\n",
    "\n",
    "reshape_form_train = (28,28)\n",
    "reshape_form_test = (28,28)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_X_train = scaler.fit_transform(X_train)\n",
    "# scaled_X_train = scaled_X_train.reshape(reshape_form_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3) ,activation = \"relu\", input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3) ,activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3,3) ,activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(128,activation = \"relu\", kernel_regularizer=l1(0.01)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(128,activation = \"relu\", kernel_regularizer=l1(0.1)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "model.summary()\n",
    "model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9829cb35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:59.527343Z",
     "iopub.status.busy": "2023-05-02T17:11:59.526287Z",
     "iopub.status.idle": "2023-05-02T17:11:59.529014Z",
     "shell.execute_reply": "2023-05-02T17:11:59.529771Z"
    },
    "id": "9829cb35"
   },
   "outputs": [],
   "source": [
    "def call_fashion_mnist():\n",
    "    global global_df\n",
    "    global test_df\n",
    "    global model\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "    # X_train = X_train/255.0\n",
    "    # X_test = X_test /255.0\n",
    "\n",
    "    global_df = pd.DataFrame(X_train)\n",
    "    global_df['Class'] = y_train\n",
    "    test_df = pd.DataFrame(X_test)\n",
    "    test_df['Class'] = y_test\n",
    "\n",
    "    reshape_form_train = (28,28)\n",
    "    reshape_form_test = (28,28)\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # scaled_X_train = scaler.fit_transform(X_train)\n",
    "    # scaled_X_train = scaled_X_train.reshape(reshape_form_train)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3) ,activation = \"relu\", input_shape = (28,28,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3,3) ,activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(128, (3,3) ,activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # model.add(Dense(128,activation = \"relu\", kernel_regularizer=l1(0.01)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(128,activation = \"relu\", kernel_regularizer=l1(0.1)))\n",
    "    # model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation = \"softmax\"))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf68bb",
   "metadata": {
    "id": "82cf68bb"
   },
   "source": [
    "<h2>Generate central node and distributed nodes to be participants </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30a37153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:59.554100Z",
     "iopub.status.busy": "2023-05-02T17:11:59.551376Z",
     "iopub.status.idle": "2023-05-02T17:11:59.584125Z",
     "shell.execute_reply": "2023-05-02T17:11:59.582409Z"
    },
    "id": "30a37153"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score\n",
    "\n",
    "\n",
    "def free_rider_experimental(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_fr(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  rounds = list(range(1, len(accuracies) + 1))\n",
    "  plt.plot(rounds, accuracies)\n",
    "  plt.grid(True)\n",
    "  plt.xlabel('Training rounds')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.show()\n",
    "    \n",
    "def free_rider_experimental_many(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 300, min_n = 300, model = model)\n",
    "  add_fr(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  rounds = list(range(1, len(accuracies) + 1))\n",
    "\n",
    "def naive_free_rider_experimental(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_fr(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  accuracies = naive_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  rounds = list(range(1, len(accuracies) + 1))\n",
    "  plt.plot(rounds, accuracies)\n",
    "  plt.grid(True)\n",
    "  plt.xlabel('Training rounds')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "def naive_free_rider_experimental_CIFAR(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central_CIFAR(300, 100000000, model, n_participants)\n",
    "  test_CIFAR(central_node)\n",
    "  distributed_nodes = generate_participants_CIFAR(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_fr_CIFAR(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  accuracies = naive_rounds_CIFAR(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "\n",
    "def free_rider_experimental_CIFAR(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central_CIFAR(300, 100000000, model, n_participants)\n",
    "  test_CIFAR(central_node)\n",
    "  distributed_nodes = generate_participants_CIFAR(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_fr_CIFAR(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  accuracies = n_rounds_CIFAR(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  rounds = list(range(1, len(accuracies) + 1))\n",
    "  plt.plot(rounds, accuracies)\n",
    "  plt.grid(True)\n",
    "  plt.xlabel('Training rounds')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.show()\n",
    "    \n",
    "def free_rider_experimental_fraud(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(100, 100000000, model, n_participants)\n",
    "  test_fraud_detection(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 1000, min_n = 200, model = model)\n",
    "  add_fr(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  accuracies = n_rounds_fd(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  plt.plot(rounds, accuracies)\n",
    "  plt.grid(True)\n",
    "  plt.xlabel('Training rounds')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "def free_rider_ma_experimental(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 4000, min_n = 4000, model = model)\n",
    "  add_fr(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  add_ma(distributed_nodes, 1, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  rounds = list(range(1, len(accuracies) + 1))\n",
    "  plt.plot(rounds, accuracies)\n",
    "  plt.xlabel('Training rounds')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def every_ma_experimental(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 1000, min_n = 1000, model = model)\n",
    "  add_fr(distributed_nodes, math.ceil(n_participants*percentage), model)\n",
    "  add_ma_each(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "  rounds = list(range(1, len(accuracies) + 1))\n",
    "  plt.plot(rounds, accuracies)\n",
    "  plt.xlabel('Training rounds')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "def ma_experimental(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_each(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_experimental_naive(model, percentage,  n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_each(distributed_nodes, model)\n",
    "  accuracies = naive_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "\n",
    "def ma_non_stealth(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_naive(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_stealth(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_stealth(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_target_visible(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_target_stealth(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_random_visible(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_random_stealth(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  accuracies = n_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_benchmark_general_naive(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  add_ma_general_naive(distributed_nodes, model)\n",
    "  accuracies = naive_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_benchmark_target_naive(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  add_ma_target_naive(distributed_nodes, model)\n",
    "  accuracies = naive_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_benchmark_general_stealth(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  add_ma_general_stealth(distributed_nodes, model)\n",
    "  accuracies = naive_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def ma_benchmark_target_stealth(model, percentage, n_participants, max_rounds):\n",
    "  central_node = Central(300, 100000000, model, n_participants)\n",
    "  test(central_node)\n",
    "  distributed_nodes = generate_participants(n_participants = math.ceil(n_participants*(1-percentage)),cost_mean = 10, cost_var = 1 , max_n = 2500, min_n = 2500, model = model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  add_ma_target_stealth(distributed_nodes, model)\n",
    "  accuracies = naive_rounds(max_rounds, central_node, distributed_nodes, 3)\n",
    "  print(\"list of accuracies over rounds\", accuracies)\n",
    "\n",
    "def check_poisoning(model, dataset):\n",
    "  test_x = test_df.drop(\"Class\", axis = 1).to_numpy()\n",
    "  test_y = test_df[\"Class\"]\n",
    "  scaler = StandardScaler()\n",
    "  test_x = scaler.fit_transform(test_x)\n",
    "  if(dataset == 'mnist'):\n",
    "    poison_label = 1\n",
    "    target_label = 9\n",
    "    test_x = test_x.reshape(-1,28,28,1)\n",
    "  elif(dataset == 'fashion_mnist'):\n",
    "    poison_label = 7\n",
    "    target_label = 9\n",
    "    test_x = test_x.reshape(-1,28,28,1)\n",
    "  else:\n",
    "    poison_label = 1\n",
    "    target_label = 9 \n",
    "    np.reshape(test_x, (test_x.shape[0], 32, 32, 3))\n",
    "\n",
    "  y_pred = model.predict(test_x)\n",
    "  y_pred = np.argmax(y_pred, axis = 1)\n",
    "  y_true = test_y\n",
    "  target_labels = ['0','1','2','3','4','5','6','7','8','9']\n",
    "  print(classification_report(y_true, y_pred, target_names=target_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6d5c5ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:11:59.598668Z",
     "iopub.status.busy": "2023-05-02T17:11:59.596803Z",
     "iopub.status.idle": "2023-05-02T17:12:38.898398Z",
     "shell.execute_reply": "2023-05-02T17:12:38.899149Z"
    },
    "id": "c6d5c5ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1\n",
    "\n",
    "cifar_10 = tf.keras.datasets.cifar10\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar_10.load_data()\n",
    "\n",
    "num_pixels = np.prod(X_train.shape[1:])\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "global_df = pd.DataFrame(X_train)\n",
    "global_df['Class'] = Y_train\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['Class'] = Y_test\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Model architecture inspired by https://www.kaggle.com/code/ukveteran/cifar10-transfer-learning-vgg16-jma/notebook\n",
    "def call_cifar():\n",
    "    global global_df\n",
    "    global test_df\n",
    "    global model\n",
    "    cifar_10 = tf.keras.datasets.cifar10\n",
    "    (X_train, Y_train), (X_test, Y_test) = cifar_10.load_data()\n",
    "    \n",
    "    num_pixels = np.prod(X_train.shape[1:])\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    global_df = pd.DataFrame(X_train)\n",
    "    global_df['Class'] = Y_train\n",
    "    test_df = pd.DataFrame(X_test)\n",
    "    test_df['Class'] = Y_test\n",
    "\n",
    "\n",
    "    adapted_input = Input(shape = (32,32,3))\n",
    "\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=False, input_tensor = adapted_input)\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "    x = model.get_layer('block3_pool').output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l1(0.001))(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l1(0.001))(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    out = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model.input, out)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19f07baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:12:38.958795Z",
     "iopub.status.busy": "2023-05-02T17:12:38.937571Z",
     "iopub.status.idle": "2023-05-02T17:12:39.031348Z",
     "shell.execute_reply": "2023-05-02T17:12:39.032680Z"
    },
    "id": "19f07baa"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import deque\n",
    "import itertools \n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "#For MNIST and Fashion MNIST\n",
    "class Central_CIFAR:\n",
    "    def __init__(self,bootstrap_n, budget, model, expected_number_of_participants):\n",
    "        \n",
    "        global global_df\n",
    "        if(len(global_df) < bootstrap_n):\n",
    "            print(\"Error occured, not enough data\")\n",
    "            return\n",
    "        \n",
    "        #Overcome delta weight attacks\n",
    "        self.epochs = [4,9,3,5]\n",
    "        self.epoch_ptr = 0\n",
    "\n",
    "        self.budget = budget\n",
    "        \n",
    "        bootstrap_data = []\n",
    "        for i in range(0,10):\n",
    "            subset = global_df[global_df[\"Class\"] == i]\n",
    "            random_sample = subset.sample(bootstrap_n//10)\n",
    "            bootstrap_data.append(random_sample)\n",
    "        self.private_data = pd.concat(bootstrap_data)\n",
    "        global_df = global_df.drop(self.private_data.index)\n",
    "        #Split the data into train and test datasets, with an equal amount of both classes\n",
    "        X = self.private_data.drop(\"Class\", axis = 1)\n",
    "        Y = self.private_data[\"Class\"]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.7, stratify = Y)\n",
    "        #scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 32, 32, 3))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 32, 32, 3))\n",
    "        \n",
    "        Y_train = keras.utils.to_categorical(Y_train, num_classes = 10)\n",
    "        Y_test = keras.utils.to_categorical(Y_test, num_classes = 10)\n",
    "        self.x_test = X_test\n",
    "        self.y_test = Y_test\n",
    "        self.threshold = 0\n",
    "        \n",
    "        self.model = model\n",
    "        with tf.device('/gpu:0'):\n",
    "            self.model.fit(X_train, Y_train, epochs = 3, validation_data = (X_test, Y_test), verbose = 0)\n",
    "\n",
    "        #For clearing GPU memory\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        self.weights = self.model.get_weights()\n",
    "        self.reward_matrix = dict()\n",
    "        \n",
    "        #For now, base reward is set arbitrarily. \n",
    "        self.base_reward = budget/(expected_number_of_participants*100)\n",
    "    \n",
    "    def flatten(self, lst):\n",
    "        flattened = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(self.flatten(item))\n",
    "            elif isinstance(item, np.ndarray):\n",
    "                flattened.extend(self.flatten(item))\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "    \n",
    "    def register(self, agent):\n",
    "        if agent.id not in self.reward_matrix:\n",
    "            self.reward_matrix[agent.id] = 0\n",
    "    \n",
    "    def broadcast(self):\n",
    "        #for now returning budget, but can add some algorithm to determine the budget for the specific round!!\n",
    "        epochs = self.epochs[self.epoch_ptr]\n",
    "        self.epoch_ptr+=1\n",
    "        if(self.epoch_ptr==4):\n",
    "          self.epoch_ptr = 0\n",
    "        return self.model.get_weights(), self.base_reward, self.reward_matrix, epochs\n",
    "    \n",
    "    def naive_aggregate(self, participant_weights):\n",
    "        flattened_pw = []\n",
    "        for each in participant_weights:\n",
    "            flattened_pw.append(self.flatten(each))\n",
    "        \n",
    "        final_weights = np.average([x for x in flattened_pw], axis = 0)\n",
    "        \n",
    "        #Update the weights of the model\n",
    "        layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "        layer_weights = []\n",
    "        head = 0\n",
    "        \n",
    "        for shape in layer_shapes:\n",
    "            size = np.prod(shape)\n",
    "            current_weight = np.array(final_weights[head:head+size])\n",
    "            layer_weights.append(current_weight.reshape(shape))\n",
    "            head += size\n",
    "        self.model.set_weights(layer_weights)\n",
    "    \n",
    "    def cluster(self, participant_weights, n_winners):\n",
    "        #For now, setting choice to be a set number\n",
    "        choice = 4\n",
    "        kmeans = KMeans(n_clusters = choice, random_state = 0)\n",
    "        flattened_pw = []\n",
    "        for each in participant_weights:\n",
    "            flattened_pw.append(self.flatten(each))\n",
    "        kmeans.fit(flattened_pw)\n",
    "        \n",
    "    \n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        #Selecting winners\n",
    "        counts = pd.Series(labels).value_counts()\n",
    "        counts_array = pd.Series(labels).value_counts().nlargest(n_winners)\n",
    "        winning_clusters = counts_array.index.tolist()\n",
    "        top_indices = np.where(np.isin(labels, winning_clusters))[0]\n",
    "        selected_parameters = []\n",
    "        for each in top_indices:\n",
    "            selected_parameters.append(flattened_pw[each])\n",
    "\n",
    "        final_weights = np.average([x for x in selected_parameters], axis = 0)\n",
    "        \n",
    "        #Update the weights of the model\n",
    "        layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "        layer_weights = []\n",
    "        head = 0\n",
    "        \n",
    "        for shape in layer_shapes:\n",
    "            size = np.prod(shape)\n",
    "            current_weight = np.array(final_weights[head:head+size])\n",
    "            layer_weights.append(current_weight.reshape(shape))\n",
    "            head += size\n",
    "        self.model.set_weights(layer_weights)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def update_model(self, final_weights):\n",
    "        layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "        layer_weights = []\n",
    "        head = 0\n",
    "        \n",
    "        for shape in layer_shapes:\n",
    "            size = np.prod(shape)\n",
    "            current_weight = np.array(final_weights[head:head+size])\n",
    "            layer_weights.append(current_weight.reshape(shape))\n",
    "            head += size\n",
    "        self.model.set_weights(layer_weights)\n",
    "        \n",
    "    \n",
    "\n",
    "    def evaluate(self, weights):\n",
    "      eval_model = self.model\n",
    "      final_weights = np.average([x for x in weights], axis = 0)\n",
    "      \n",
    "      #Update the weights of the model\n",
    "      layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "      layer_weights = []\n",
    "      head = 0\n",
    "      for shape in layer_shapes:\n",
    "          size = (np.prod(shape))\n",
    "          current_weight = np.array(final_weights[head:head+size])\n",
    "          layer_weights.append(current_weight.reshape(shape))\n",
    "          head += (size)\n",
    "      eval_model.set_weights(layer_weights)\n",
    "\n",
    "      return eval_model.evaluate(self.x_test, self.y_test, verbose = 0)[1]\n",
    "\n",
    "        \n",
    "    def strong_cluster(self, participant_weights, n_winners, p_ids):\n",
    "      choice = 5\n",
    "      selection_max = len(participant_weights) * 0.5\n",
    "      while(True):\n",
    "        no_doubles = 1\n",
    "        if(len(participant_weights) < choice):\n",
    "            print(\"clustering failed due to not enough participants\")\n",
    "            return\n",
    "        kmeans = KMeans(n_clusters = choice, random_state = 0, n_init = 10)\n",
    "        flattened_pw = []\n",
    "        for each in participant_weights:\n",
    "            flattened_pw.append(self.flatten(each))\n",
    "        # scaler = StandardScaler()\n",
    "        # scaler.fit(flattened_pw)\n",
    "        # scaled_pw = scaler.transform(flattened_pw)\n",
    "        kmeans.fit(flattened_pw)\n",
    "        \n",
    "    \n",
    "        labels = np.unique(kmeans.labels_)\n",
    "\n",
    "        #Testing purposes only! Remove when not testing\n",
    "        \n",
    "        # Shows ID's of agents in each cluster\n",
    "        label_list = [[] for i in range(max(kmeans.labels_) + 1)]\n",
    "        indices = [[] for i in range(len(labels)+1)]\n",
    "        scoring = [[] for i in range(len(labels)+1)]\n",
    "        remaining_count = 0\n",
    "        for i, label in enumerate(kmeans.labels_):\n",
    "          label_list[label].append(i)\n",
    "          indices[label].append(p_ids[i])\n",
    "          scoring[label].append(i)\n",
    "          remaining_count +=1\n",
    "        print(\"clustering distribution\", indices)\n",
    "        \n",
    "        \n",
    "        accs = []\n",
    "        for i in scoring:\n",
    "          if(len(i) == 0):\n",
    "            continue\n",
    "          accs.append(self.evaluate([flattened_pw[x] for x in i]))\n",
    "        print(\"individual acc:\", accs)\n",
    "        #End of accuracy testing\n",
    "\n",
    "        #Selecting winners\n",
    "        \n",
    "\n",
    "        #Remove worst cluster\n",
    "        bottom_cluster = np.argmin(accs)\n",
    "        removed_cluster = [x for x in label_list[bottom_cluster]]\n",
    "        label_list[bottom_cluster] = []\n",
    "\n",
    "        #Remove from the weightings at the end of the loop\n",
    "        removal_list = [x for x in label_list if len(x)<=0.1*remaining_count]\n",
    "        removal_list.append(removed_cluster)\n",
    "        removed_labels = [i for i, x in enumerate(label_list) if len(x)<=0.1*remaining_count]\n",
    "        \n",
    "        labels = [x for x in labels if x not in removed_labels]\n",
    "        print(labels)\n",
    "        \n",
    "        removal_list = [x for i in removal_list for x in i]\n",
    "        removal_list = sorted(removal_list, reverse= True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #Removing clusters with very few members - tau = 0.1n\n",
    "        label_list = [x if len(x)>0.1*remaining_count else [] for x in label_list]\n",
    "\n",
    "\n",
    "\n",
    "        #updating remaining count of participants\n",
    "        remaining_count -= len(removal_list)\n",
    "\n",
    "        #Count clusters remaining\n",
    "        cluster_count = len(labels)\n",
    "        #End clustering if no remaining clusters\n",
    "        if(cluster_count == 0):\n",
    "          print(\"Error occured, no remaining clusters\")\n",
    "          return\n",
    "\n",
    "        #Need to recluster if cluster count is less than Z_min (we set Z_min to 3)\n",
    "        if(cluster_count < 3 and remaining_count>5):\n",
    "          print(\"reclustering\")\n",
    "          for i in removal_list:\n",
    "            participant_weights.pop(i)\n",
    "            p_ids.pop(i)\n",
    "          continue\n",
    "\n",
    "        #In case cluster count is now too small\n",
    "        if(cluster_count<n_winners):\n",
    "          n_winners = cluster_count\n",
    "        \n",
    "\n",
    "        print(label_list)\n",
    "        #- -- - - - - - This section implements Shapley Value - - - -- -  -\n",
    "        possible_coalitions = []\n",
    "        for i in range(1, cluster_count+1):\n",
    "            for combination in itertools.combinations(labels, i):\n",
    "                possible_coalitions.append(combination)\n",
    "        shapley = np.zeros(choice)\n",
    "        print(possible_coalitions)\n",
    "        for i in labels:\n",
    "            for coalition in possible_coalitions:\n",
    "                if i in coalition:\n",
    "                    if(len(coalition)==1):\n",
    "                        shapley[i]+= self.evaluate([flattened_pw[x] for x in label_list[i]])\n",
    "                    else:\n",
    "                        other_clusters = [label for label in coalition if label!=i]\n",
    "                        other_clusters_pw = [flattened_pw[x] for c in other_clusters for x in label_list[c]]\n",
    "                        full_coalition_pw = [flattened_pw[x] for c in coalition for x in label_list[c]]\n",
    "                        marginal = self.evaluate(full_coalition_pw) - self.evaluate(other_clusters_pw)\n",
    "                        shapley[i] += marginal\n",
    "                        \n",
    "        \n",
    "        print(\"shapley: \", shapley)\n",
    "        #------------- End of section that implements Shapley Value ------------\n",
    "        selected_labels = [cluster_label for cluster_label in labels if shapley[cluster_label] > 0]\n",
    "        selected_indices = [x for i in selected_labels for x in label_list[i]]\n",
    "        winner_ids = [p_ids[i] for i in selected_indices]\n",
    "        print(\"These are the winners\", winner_ids)\n",
    "        selected_parameters = [flattened_pw[x] for x in selected_indices]\n",
    "        break\n",
    "        \n",
    "        #------------- Selection scheme that uses Shapley Value ---------------\n",
    "        \n",
    "\n",
    "      final_weights = np.average([x for x in selected_parameters], axis = 0)\n",
    "      \n",
    "      #Update the weights of the model\n",
    "      layer_shapes = [w.shape for w in self.model.get_weights()]\n",
    "      layer_weights = []\n",
    "      head = 0\n",
    "      \n",
    "      for shape in layer_shapes:\n",
    "          size = np.prod(shape)\n",
    "          current_weight = np.array(final_weights[head:head+size])\n",
    "          layer_weights.append(current_weight.reshape(shape))\n",
    "          head += size\n",
    "      self.model.set_weights(layer_weights)\n",
    "#       self.threshold = self.model.evaluate(self.x_test, self.y_test, verbose = 0)[1]\n",
    "#       print(\"this is performance on own dataset\", self.threshold)\n",
    "\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "  \n",
    "\n",
    "class Participant_CIFAR:\n",
    "    def __init__(self, data_size, participation_cost, identity, model):\n",
    "        \n",
    "        global global_df\n",
    "        if(len(global_df) < data_size):\n",
    "            print(\"Error occured, not enough data\")\n",
    "            return\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.participation_cost = participation_cost\n",
    "        self.weights = []\n",
    "        self.id = identity\n",
    "        self.local_model = tf.keras.models.clone_model(model)\n",
    "        self.epochs = 5\n",
    "        \n",
    "        bootstrap_data = []\n",
    "        for i in range(0,10):\n",
    "            subset = global_df[global_df[\"Class\"] == i]\n",
    "            random_sample = subset.sample(data_size//10)\n",
    "            bootstrap_data.append(random_sample)\n",
    "        self.private_data = pd.concat(bootstrap_data)\n",
    "        global_df = global_df.drop(self.private_data.index)\n",
    "        \n",
    "        \n",
    "    def test_data(self):\n",
    "        print(self.data)\n",
    "\n",
    "    #Need to make a function to decide whether participant will bid to take part in particular round\n",
    "    def make_bid(self):\n",
    "        return 0\n",
    "\n",
    "    def train(self, model_weights, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.local_model.set_weights(model_weights)\n",
    "        self.local_model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "               #Split the data into train and test datasets, with an equal amount of both classes\n",
    "        X = self.private_data.drop(\"Class\", axis = 1)\n",
    "        Y = self.private_data[\"Class\"]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y)\n",
    "        #scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        num_pixels = np.prod(X_train.shape[1:])\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 32, 32, 3))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 32, 32, 3))\n",
    "        Y_train = keras.utils.to_categorical(Y_train, num_classes = 10)\n",
    "        Y_test = keras.utils.to_categorical(Y_test, num_classes = 10)\n",
    "        self.local_model.fit(X_train, Y_train,batch_size = 16, epochs = self.epochs, validation_data = (X_test, Y_test), verbose = 0)\n",
    "\n",
    "        self.weights = []\n",
    "        for layer in self.local_model.layers:\n",
    "            layer_w = layer.get_weights()\n",
    "            for w in layer_w:\n",
    "                self.weights.append(w)\n",
    "    \n",
    "    def participate(self, round_reward):\n",
    "        #A simple expected value to decide if it is worth participating - can change to more complicated model soon\n",
    "        if(round_reward > self.participation_cost):\n",
    "            return True\n",
    "    \n",
    "    def submit(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getID(self):\n",
    "      return self.id\n",
    "\n",
    "class Free_Rider_CIFAR:\n",
    "    \n",
    "    def __init__(self, strategy, identity, model):\n",
    "        self.id = identity\n",
    "        self.strategy = strategy\n",
    "        self.local_model = tf.keras.models.clone_model(model)\n",
    "        #History used for delta weight attacks (estimate weight updates)\n",
    "        self.history = None\n",
    "    \n",
    "    def participate(self, round_reward):\n",
    "        return True\n",
    "    \n",
    "    def train(self, model_weights, epochs):\n",
    "        self.local_model.set_weights(model_weights)\n",
    "        self.local_model.compile(optimizer = Adam(learning_rate=0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "        if self.strategy==1:\n",
    "            #Simply return the global model weights - not used in experiments shown in conference paper\n",
    "            self.weights = []\n",
    "            for layer in self.local_model.layers:\n",
    "                layer_w = layer.get_weights()\n",
    "                for w in layer_w:\n",
    "                    self.weights.append(w)\n",
    "                    \n",
    "        #Random arbitrary noise - not used in experiments shown in conference paper\n",
    "        elif self.strategy == 2:\n",
    "            #return the weights with some arbitrary noise added\n",
    "            noise = random.uniform(random.randint(-15,0),random.randint(1,15))\n",
    "            self.weights = []\n",
    "            for layer in self.local_model.layers:\n",
    "                    layer_w = layer.get_weights()\n",
    "                    for w in layer_w:\n",
    "                        self.weights.append(w+noise)\n",
    "                        \n",
    "        #Gaussian noise\n",
    "        elif self.strategy == 3:\n",
    "            #return the weights with some arbitrary Gaussian noise added\n",
    "            self.weights = []\n",
    "            for layer in self.local_model.layers:\n",
    "                layer_w = layer.get_weights()\n",
    "                for w in layer_w:\n",
    "                    noisy_weights = random.normalvariate(w, w*random.uniform(0, 1.5))\n",
    "                    self.weights.append(noisy_weights)\n",
    "        \n",
    "        #Delta weight attacks\n",
    "        elif self.strategy == 4:\n",
    "            if(self.history == None):\n",
    "                self.history = tf.keras.models.clone_model(self.local_model)\n",
    "                #not enough history yet\n",
    "                self.weights = []\n",
    "                for layer in self.local_model.layers:\n",
    "                    layer_w = layer.get_weights()\n",
    "                    for w in layer_w:\n",
    "                        noisy_weights = random.normalvariate(w, w*random.uniform(0, 1.5))\n",
    "                        self.weights.append(noisy_weights)\n",
    "            else:\n",
    "                self.weights = [] \n",
    "                previous_model = self.history\n",
    "                for layer1, layer2 in zip(self.local_model.layers, previous_model.layers):\n",
    "                    layer_w1 = layer1.get_weights()\n",
    "                    layer_w2 = layer2.get_weights()\n",
    "                    for w1, w2 in zip(layer_w1, layer_w2):\n",
    "                        delta_weights = w1 + (w1-w2)\n",
    "                        delta_noise = random.normalvariate(0, abs(w1-w2)/5)\n",
    "                        self.weights.append(delta_weights + delta_noise)\n",
    "            self.history.set_weights(model_weights)\n",
    "                    \n",
    "    def submit(self):\n",
    "        return self.weights\n",
    "      \n",
    "    def getID(self):\n",
    "      return self.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d73f2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:12:39.040126Z",
     "iopub.status.busy": "2023-05-02T17:12:39.038663Z",
     "iopub.status.idle": "2023-05-02T17:31:38.870228Z",
     "shell.execute_reply": "2023-05-02T17:31:38.871638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,001,994\n",
      "Trainable params: 265,994\n",
      "Non-trainable params: 1,736,000\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 20.6997 - accuracy: 0.1157\n",
      "20.699689865112305 0.11569999903440475\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 10.3384 - accuracy: 0.5550\n",
      "10.33835506439209 0.5550000071525574\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 7.2704 - accuracy: 0.2548\n",
      "7.270405292510986 0.2547999918460846\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2277 - accuracy: 0.6422\n",
      "5.227662086486816 0.6421999931335449\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.7127 - accuracy: 0.6455\n",
      "4.712719440460205 0.6455000042915344\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.6141 - accuracy: 0.6508\n",
      "3.6140897274017334 0.6507999897003174\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.6343 - accuracy: 0.6391\n",
      "2.6343488693237305 0.6391000151634216\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.0170 - accuracy: 0.6531\n",
      "2.017011880874634 0.6531000137329102\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.7782 - accuracy: 0.6493\n",
      "1.778220534324646 0.6492999792098999\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.8897 - accuracy: 0.6482\n",
      "1.8897136449813843 0.6481999754905701\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4814 - accuracy: 0.6568\n",
      "1.4814341068267822 0.6567999720573425\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.4460 - accuracy: 0.6606\n",
      "1.4460487365722656 0.6606000065803528\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4393 - accuracy: 0.6468\n",
      "1.4392825365066528 0.6467999815940857\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4733 - accuracy: 0.6427\n",
      "1.4733165502548218 0.6427000164985657\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.3274 - accuracy: 0.6652\n",
      "1.3273910284042358 0.6651999950408936\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2667 - accuracy: 0.6752\n",
      "1.2667458057403564 0.6751999855041504\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.2489 - accuracy: 0.6783\n",
      "1.248910665512085 0.6783000230789185\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2349 - accuracy: 0.6810\n",
      "1.2349125146865845 0.6809999942779541\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2149 - accuracy: 0.6866\n",
      "1.2148686647415161 0.6866000294685364\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2034 - accuracy: 0.6875\n",
      "1.2034265995025635 0.6875\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.2473 - accuracy: 0.6799\n",
      "1.2472922801971436 0.6798999905586243\n",
      "list of accuracies over rounds [0.5550000071525574, 0.2547999918460846, 0.6421999931335449, 0.6455000042915344, 0.6507999897003174, 0.6391000151634216, 0.6531000137329102, 0.6492999792098999, 0.6481999754905701, 0.6567999720573425, 0.6606000065803528, 0.6467999815940857, 0.6427000164985657, 0.6651999950408936, 0.6751999855041504, 0.6783000230789185, 0.6809999942779541, 0.6866000294685364, 0.6875, 0.6798999905586243]\n"
     ]
    }
   ],
   "source": [
    "call_cifar()\n",
    "naive_free_rider_experimental_CIFAR(model, 0.1, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a84e2668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:31:38.885013Z",
     "iopub.status.busy": "2023-05-02T17:31:38.883487Z",
     "iopub.status.idle": "2023-05-02T17:48:49.206243Z",
     "shell.execute_reply": "2023-05-02T17:48:49.207609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,001,994\n",
      "Trainable params: 265,994\n",
      "Non-trainable params: 1,736,000\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 18.1298 - accuracy: 0.1629\n",
      "18.12976837158203 0.16290000081062317\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 11.0313 - accuracy: 0.5617\n",
      "11.031303405761719 0.5616999864578247\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 9.3191 - accuracy: 0.5999\n",
      "9.319148063659668 0.5999000072479248\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 10.6070 - accuracy: 0.3370\n",
      "10.607014656066895 0.3370000123977661\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 9.3940 - accuracy: 0.5437\n",
      "9.39400577545166 0.5436999797821045\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 9.5171 - accuracy: 0.5300\n",
      "9.51707935333252 0.5299999713897705\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 9.5660 - accuracy: 0.6095\n",
      "9.566006660461426 0.609499990940094\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 9.7086 - accuracy: 0.6177\n",
      "9.708563804626465 0.6176999807357788\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 8.9712 - accuracy: 0.6506\n",
      "8.971247673034668 0.650600016117096\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 8.8389 - accuracy: 0.6747\n",
      "8.838874816894531 0.6747000217437744\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.6012 - accuracy: 0.6848\n",
      "6.601203918457031 0.6848000288009644\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 6.5386 - accuracy: 0.6311\n",
      "6.538597106933594 0.6310999989509583\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2594 - accuracy: 0.6641\n",
      "5.259407043457031 0.6640999913215637\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.1737 - accuracy: 0.6515\n",
      "4.173696041107178 0.6514999866485596\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 3.7129 - accuracy: 0.4804\n",
      "3.7129030227661133 0.4803999960422516\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 3.5041 - accuracy: 0.6275\n",
      "3.5041120052337646 0.6274999976158142\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.0340 - accuracy: 0.6275\n",
      "3.034008264541626 0.6274999976158142\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 2.6133 - accuracy: 0.6175\n",
      "2.613332748413086 0.6175000071525574\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 3.8345 - accuracy: 0.2577\n",
      "3.834462881088257 0.25769999623298645\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 3.6990 - accuracy: 0.1799\n",
      "3.698974132537842 0.17990000545978546\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.4394 - accuracy: 0.4510\n",
      "2.439432144165039 0.45100000500679016\n",
      "list of accuracies over rounds [0.5616999864578247, 0.5999000072479248, 0.3370000123977661, 0.5436999797821045, 0.5299999713897705, 0.609499990940094, 0.6176999807357788, 0.650600016117096, 0.6747000217437744, 0.6848000288009644, 0.6310999989509583, 0.6640999913215637, 0.6514999866485596, 0.4803999960422516, 0.6274999976158142, 0.6274999976158142, 0.6175000071525574, 0.25769999623298645, 0.17990000545978546, 0.45100000500679016]\n"
     ]
    }
   ],
   "source": [
    "call_cifar()\n",
    "naive_free_rider_experimental_CIFAR(model, 0.25, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ab95571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T17:48:49.219193Z",
     "iopub.status.busy": "2023-05-02T17:48:49.217747Z",
     "iopub.status.idle": "2023-05-02T18:00:31.330682Z",
     "shell.execute_reply": "2023-05-02T18:00:31.331981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,001,994\n",
      "Trainable params: 265,994\n",
      "Non-trainable params: 1,736,000\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 18.6229 - accuracy: 0.2041\n",
      "18.622852325439453 0.20409999787807465\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 13.2322 - accuracy: 0.3724\n",
      "13.23222541809082 0.3723999857902527\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 10.0764 - accuracy: 0.5567\n",
      "10.076375961303711 0.5566999912261963\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 6.6095 - accuracy: 0.6229\n",
      "6.609539031982422 0.6229000091552734\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.7527 - accuracy: 0.5124\n",
      "4.752719402313232 0.5123999714851379\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.7722 - accuracy: 0.5800\n",
      "3.7722249031066895 0.5799999833106995\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 8.4212 - accuracy: 0.1569\n",
      "8.42123031616211 0.15690000355243683\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.8664 - accuracy: 0.1064\n",
      "5.866413116455078 0.10639999806880951\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.7914 - accuracy: 0.1442\n",
      "3.791407823562622 0.14419999718666077\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.5821 - accuracy: 0.1000\n",
      "2.582066774368286 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 3.0933 - accuracy: 0.1003\n",
      "3.093343496322632 0.10029999911785126\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.2737 - accuracy: 0.1000\n",
      "3.2737131118774414 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.9207 - accuracy: 0.1000\n",
      "2.9206981658935547 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "list of accuracies over rounds [0.3723999857902527, 0.5566999912261963, 0.6229000091552734, 0.5123999714851379, 0.5799999833106995, 0.15690000355243683, 0.10639999806880951, 0.14419999718666077, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10029999911785126, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "call_cifar()\n",
    "naive_free_rider_experimental_CIFAR(model, 0.5, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "578e6633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:00:31.343329Z",
     "iopub.status.busy": "2023-05-02T18:00:31.341874Z",
     "iopub.status.idle": "2023-05-02T18:08:10.874942Z",
     "shell.execute_reply": "2023-05-02T18:08:10.876428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,001,994\n",
      "Trainable params: 265,994\n",
      "Non-trainable params: 1,736,000\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 21.9572 - accuracy: 0.1045\n",
      "21.95722198486328 0.10450000315904617\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 18.7040 - accuracy: 0.1024\n",
      "18.704021453857422 0.10239999741315842\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 13.3724 - accuracy: 0.1891\n",
      "13.372445106506348 0.1890999972820282\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 12.3479 - accuracy: 0.1017\n",
      "12.347938537597656 0.10170000046491623\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 10.8155 - accuracy: 0.1478\n",
      "10.815539360046387 0.1477999985218048\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.4104 - accuracy: 0.1000\n",
      "7.410409450531006 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 7.3948 - accuracy: 0.1000\n",
      "7.3947834968566895 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 6.0950 - accuracy: 0.1000\n",
      "6.094980716705322 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 5.3308 - accuracy: 0.1000\n",
      "5.3307785987854 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 7.4039 - accuracy: 0.1000\n",
      "7.403893947601318 0.10000000149011612\n",
      "313/313 [==============================] - 2s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 9ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1000\n",
      "nan 0.10000000149011612\n",
      "list of accuracies over rounds [0.10239999741315842, 0.1890999972820282, 0.10170000046491623, 0.1477999985218048, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "call_cifar()\n",
    "naive_free_rider_experimental_CIFAR(model, 0.75, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9d705dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:08:10.898601Z",
     "iopub.status.busy": "2023-05-02T18:08:10.896752Z",
     "iopub.status.idle": "2023-05-02T18:08:10.902557Z",
     "shell.execute_reply": "2023-05-02T18:08:10.901057Z"
    },
    "id": "b9d705dc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
